# ============================================================
# LLM Configuration (AWS Bedrock is preferred, OpenAI as fallback)
# ============================================================

# AWS Bedrock (Primary - Recommended for AWS deployments)
AWS_REGION_NAME="us-west-2"  # Region for Bedrock (us-west-2 recommended for Claude 4.5 Haiku)
AWS_ACCESS_KEY_ID=""  # Your AWS access key (required for Bedrock)
AWS_SECRET_ACCESS_KEY=""  # Your AWS secret key (required for Bedrock)
AWS_SESSION_TOKEN=""  # Optional: For temporary credentials
AWS_MODEL_ARN="us.anthropic.claude-sonnet-4-5-20250929-v1:0"
AWS_MODEL_ID="us.anthropic.claude-sonnet-4-5-20250929-v1:0"

# OpenAI (Fallback if AWS Bedrock is not configured)
OPENAI_API_KEY=""
OPENAI_MODEL="gpt-5-mini"  # Default OpenAI model
OPENAI_BASE_URL=""  # Optional

# ============================================================
# Optional LangSmith tracing and experiment tracking
# ============================================================
LANGSMITH_API_KEY=""
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_TRACING="false"
LANGSMITH_PROJECT="aws-shopping-agent"

# ============================================================
# Alternative LLM Providers (Optional)
# ============================================================

# Azure OpenAI
AZURE_OPENAI_API_KEY=""
AZURE_OPENAI_ENDPOINT=""
AZURE_OPENAI_API_VERSION=""

# Anthropic (Direct, not via Bedrock)
ANTHROPIC_API_KEY=""

# OpenSearch Configuration
# For local development with Docker
OPENSEARCH_HOST="localhost"
OPENSEARCH_PORT="9200"
OPENSEARCH_USE_SSL="false"
OPENSEARCH_VERIFY_CERTS="false"
OPENSEARCH_USERNAME=""  # Empty for local Docker with security disabled
OPENSEARCH_PASSWORD=""  # Empty for local Docker with security disabled

# For Amazon OpenSearch Service (production)
# OPENSEARCH_HOST="your-domain.us-east-1.es.amazonaws.com"
# OPENSEARCH_PORT="443"
# OPENSEARCH_USE_SSL="true"
# OPENSEARCH_VERIFY_CERTS="true"
# OPENSEARCH_USERNAME="admin"
# OPENSEARCH_PASSWORD="your-secure-password"

# OpenSearch Index Configuration
OPENSEARCH_INDEX_PRODUCTS="shopping_products"
OPENSEARCH_MODEL_ID=""  # Will be populated after model deployment

# OpenSearch Agentic Memory Configuration
# Memory container ID for customer preferences (run setup_opensearch_memory_container.py)
OPENSEARCH_MEMORY_CONTAINER_ID=""  # Will be populated after memory container setup
# Optional: LLM model ID for memory processing (enables long-term memory features)
OPENSEARCH_LLM_MODEL_ID=""  # Optional, improves memory summarization
