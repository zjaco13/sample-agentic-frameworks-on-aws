{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd4f701",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/map-reduce.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239947-lesson-3-map-reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36737349-c949-4d64-9aa3-3767cbd02ad1",
   "metadata": {},
   "source": [
    "# Map-reduce\n",
    "\n",
    "## Review\n",
    "\n",
    "We're building up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "To build this multi-agent assistant, we've been introducing a few LangGraph controllability topics.\n",
    "\n",
    "We just covered parallelization and sub-graphs.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're going to cover [map reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_aws langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd868a",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe9b9f-4375-4bca-8e32-7d57cb861469",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Map-reduce operations are essential for efficient task decomposition and parallel processing. \n",
    "\n",
    "It has two phases:\n",
    "\n",
    "(1) `Map` - Break a task into smaller sub-tasks, processing each sub-task in parallel.\n",
    "\n",
    "(2) `Reduce` - Aggregate the results across all of the completed, parallelized sub-tasks.\n",
    "\n",
    "Let's design a system that will do two things:\n",
    "\n",
    "(1) `Map` - Create a set of jokes about a topic.\n",
    "\n",
    "(2) `Reduce` - Pick the best joke from the list.\n",
    "\n",
    "We'll use an LLM to do the job generation and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994cf903-1ed6-4ae2-b32a-7891a2808f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Prompts we will use\n",
    "subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\n",
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n",
    "\n",
    "# LLM\n",
    "model = ChatBedrock(model=\"us.amazon.nova-pro-v1:0\", temperature=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b883cc-3469-4e96-b1a4-deadf7bf3ce5",
   "metadata": {},
   "source": [
    "## State\n",
    "\n",
    "### Parallelizing joke generation\n",
    "\n",
    "First, let's define the entry point of the graph that will:\n",
    "\n",
    "* Take a user input topic\n",
    "* Produce a list of joke topics from it\n",
    "* Send each joke topic to our above joke generation node\n",
    "\n",
    "Our state has a `jokes` key, which will accumulate jokes from parallelized joke generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099218ca-ee78-4291-95a1-87ee61382e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int\n",
    "    \n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7176d1c-4a88-4b0f-a960-ee04a45279bd",
   "metadata": {},
   "source": [
    "Generate subjects for jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45010efd-ad31-4daa-b77e-aaec79ef0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topics(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5296bb0-c163-4e5c-8181-1e305b37442a",
   "metadata": {},
   "source": [
    "Here is the magic: we use the [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) to create a joke for each subject.\n",
    "\n",
    "This is very useful! It can automatically parallelize joke generation for any number of subjects.\n",
    "\n",
    "* `generate_joke`: the name of the node in the graph\n",
    "* `{\"subject\": s`}: the state to send\n",
    "\n",
    "`Send` allow you to pass any state that you want to `generate_joke`! It does not have to align with `OverallState`.\n",
    "\n",
    "In this case, `generate_joke` is using its own internal state, and we can populate this via `Send`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc83e575-11f6-41a9-990a-adb571bcda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847192d-d358-411e-90c0-f06be0738717",
   "metadata": {},
   "source": [
    "### Joke generation (map)\n",
    "\n",
    "Now, we just define a node that will create our jokes, `generate_joke`!\n",
    "\n",
    "We write them back out to `jokes` in `OverallState`! \n",
    "\n",
    "This key has a reducer that will combine lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcddc567-73d3-4fb3-bfc5-1bea538f2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    return {\"jokes\": [response.joke]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02960657-d174-4076-99a8-b3f9eea015f4",
   "metadata": {},
   "source": [
    "### Best joke selection (reduce)\n",
    "\n",
    "Now, we add logic to pick the best joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d672870-75e3-4307-bda0-c41a86cbbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_joke(state: OverallState):\n",
    "    jokes = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cd12e-5bff-426e-97f4-c774df998cfb",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae6be4b-144e-483c-88ad-ce86d6477a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# Construct the graph: here we put everything together to construct our graph\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_topics\", generate_topics)\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "graph.add_edge(START, \"generate_topics\")\n",
    "graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e21dc7c9-0add-4125-be76-af701adb874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_topics': {'subjects': ['wildlife', 'domestic pets', 'animal behavior']}}\n",
      "{'generate_joke': {'jokes': [\"Why did the deer stop in the forest? Because it couldn't find the exit sign!\"]}}\n",
      "{'generate_joke': {'jokes': [\"Why don't some animals play cards? Because there's no poker face!\"]}}\n",
      "{'generate_joke': {'jokes': ['Why did the cat sit on the computer? To keep an eye on the mouse!']}}\n"
     ]
    },
    {
     "ename": "ThrottlingException",
     "evalue": "An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 4): Too many requests, please wait before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mThrottlingException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Call the graph: here we call it to generate a list of jokes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manimals\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mbest_joke\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      2\u001b[39m jokes = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(state[\u001b[33m\"\u001b[39m\u001b[33mjokes\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m prompt = best_joke_prompt.format(topic=state[\u001b[33m\"\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m\"\u001b[39m], jokes=jokes)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBestJoke\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mbest_selected_joke\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mjokes\u001b[39m\u001b[33m\"\u001b[39m][response.id]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:5431\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5424\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5426\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5429\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5430\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5432\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5435\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_aws/chat_models/bedrock_converse.py:660\u001b[39m, in \u001b[36mChatBedrockConverse._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    658\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    659\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mUsing Bedrock Converse API to generate response\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbedrock_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse from Bedrock: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    664\u001b[39m response_message = _parse_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/botocore/client.py:570\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    567\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m     )\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/botocore/context.py:124\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    123\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/botocore/client.py:1031\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1027\u001b[39m     error_code = error_info.get(\u001b[33m\"\u001b[39m\u001b[33mQueryErrorCode\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\n\u001b[32m   1028\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m     )\n\u001b[32m   1030\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mThrottlingException\u001b[39m: An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 4): Too many requests, please wait before trying again.",
      "During task with name 'best_joke' and id '6d7d733e-5639-4bc0-4f5e-43bb7bd13e66'"
     ]
    }
   ],
   "source": [
    "# Call the graph: here we call it to generate a list of jokes\n",
    "for s in app.stream({\"topic\": \"animals\"}):\n",
    "    print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a96517e-77ab-46e2-95e2-79168c044e9c",
   "metadata": {},
   "source": [
    "## Studio\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- üöÄ API: http://127.0.0.1:2024\n",
    "- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- üìö API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "Let's load our the above graph in the Studio UI, which uses `module-4/studio/map_reduce.py` set in `module-4/studio/langgraph.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5e45-9a4c-43b4-8393-9298b3dcda53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lc-academy-env)",
   "language": "python",
   "name": "lc-academy-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
