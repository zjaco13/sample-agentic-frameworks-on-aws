{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain-aws langgraph_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f02cce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatBedrock(model=\"us.amazon.nova-pro-v1:0\") \n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content=\"Hello, Lance! It's nice to meet you. How can I assist you today? Are you looking for information on a specific topic, need help with a problem, or just have a question? Let me know how I can help!\", additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'e2502316-4a98-4f7f-bcfe-5e9b3b2d169f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 09 Jun 2025 18:44:34 GMT', 'content-type': 'application/json', 'content-length': '376', 'connection': 'keep-alive', 'x-amzn-requestid': 'e2502316-4a98-4f7f-bcfe-5e9b3b2d169f'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [807]}, 'model_name': 'us.amazon.nova-pro-v1:0'}, id='run--6d0ea6ee-0f25-49d1-9257-700b02841961-0', usage_metadata={'input_tokens': 6, 'output_tokens': 49, 'total_tokens': 55})}}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello again, Lance! It's nice to connect with you. If you have any questions, need information, or just want to chat about something specific, let me know. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "---------------------------------------------------------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Lance! It's nice to meet you. How can I assist you today? If you have a question, need information on a particular topic, or require help with something specific, feel free to ask.\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event['messages']:\n",
    "        m.pretty_print()\n",
    "    print(\"---\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatBedrock\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content=[], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'Certainly', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '! The San', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Francisco 4', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '9ers are a professional', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' American football team based', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in the San', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Francisco Bay Area', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '. They', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' compete', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in the National', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Football League (NFL', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ') as a', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' member club', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' of the National', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Football Conference', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' (N', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'FC) West division', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '. Hereâ€™', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 's an', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' overview of the team', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':\\n\\n### History', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n- **Found', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'ing', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':**', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' The 4', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '9ers were founded in ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '1946', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' as', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' a', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' charter member', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' of the All', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '-', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'America', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Football', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Conference (AAF', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'C', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ') before joining', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the NFL', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in 1949', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n- **Name', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Origin', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** The team is', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' named after', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the prospect', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'ors who arrived', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in Northern', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' California in 18', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '49', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' during the Gold', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Rush.\\n\\n### Championships', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '- The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '49ers have', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' won five', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Super Bowl titles', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' (', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'X', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'VI', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', XIX,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' XXIII', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', XXI', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'V', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', XXIX', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '), tying', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' them with', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the Dallas', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Cowboys and', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Pittsburgh Steelers for the second', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '-', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'most Super Bowl victories', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in history', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ',', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' behind the New', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' England Patriotsâ€™', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' six', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n\\n', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '### Notable', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Era', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 's', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n1', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '. **1', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '95', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '0s-', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '197', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '0s:** The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' team', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' had', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' moderate', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' success but', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' did', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' not reach', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the same', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' heights', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' as they', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' would in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' later decades', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n2. **19', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '80s:**', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' This', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' was', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' team', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'â€™', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 's golden era,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' largely', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' due', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' to the leadership', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' of head', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' coach Bill', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Walsh', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and quarterback Joe Montana.', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' They won four', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Super Bowls during this', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' decade.\\n3. **1', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '990s:** The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '49ers continued their', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' success into', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the 199', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '0s,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' winning their', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' fifth Super Bowl in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 1', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '994', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' with', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Steve', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Young', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' at', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' quarterback.\\n4. **2', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '000', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 's:**', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' A', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' period', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' of decline', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' rebuilding, marked', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' by frequent', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' coaching', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' changes and', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' mediocre', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' performance', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n5. **20', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '10s:** A', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' resurgence under', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' coach', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Jim', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Harbaugh', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ',', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' reaching', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Super Bowl in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 2011', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 2012 seasons', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ',', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' though they lost', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' both', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' times', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n\\n### Key', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Players\\n- **Joe', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Montana:** Considered', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' one of the greatest', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' quarterbacks', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' NFL history', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', Montana', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' led the ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '49ers to four', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Super Bowl victories', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n- **Jerry', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Rice', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** Widely', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' regarded as the greatest', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' wide', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' receiver of', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' all time', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', Rice', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' holds', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' many', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' NFL receiving', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' records.\\n- **Steve Young', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** Montana', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'â€™s successor,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Young', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' led', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the team to its', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' fifth', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Super Bowl win', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '- **Patrick', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Willis', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** A dominant', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' linebacker in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the 200', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '0s and', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '2010s,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Willis', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' was a key defensive', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' player.\\n\\n', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '### Stadiums', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n- **Ke', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'zar', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Stadium', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** The team', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'â€™', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 's original', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' home from', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 194', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '6 to 197', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '0', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n- **Candle', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'stick', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Park', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** Home', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' from', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 197', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '1 to 2', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '013,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' known', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' for its cold', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', windy conditions', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n- **Lev', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'i', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'â€™s Stadium', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ':** The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '49ersâ€™ current', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' home,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' opened in 201', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '4 in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Santa Clara', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ', California', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n\\n### Recent', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Years', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n- The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 4', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '9ers have seen', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' a mix of success', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and challenges', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' recent years. They', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' reached', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Super Bowl LIV', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in the 201', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '9 season but lost', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' to the Kansas', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' City Chiefs.\\n- The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' team', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' is', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' known', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' for its strong', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' defense', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and has', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' been', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' rebuilding', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' its', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' roster', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' with a focus', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' on both', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' offense', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and defense', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\\n\\n### Fan', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Base', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n- The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' ', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '49ers have a passionate', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and loyal', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' fan base,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' often', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' referred to as', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' \"Gold', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Rush', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.\"', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Their', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' games', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' are known for', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' electric', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' atmosphere,', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' especially at', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Levi', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': 'â€™s Stadium.\\n\\n###', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Future', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' Outlook', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '\\n- The', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' 4', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '9ers continue', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' to be', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' a competitive team', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in the NFL, with a', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' focus on developing', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' young', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' talent and making', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' strategic moves in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the draft', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' and free', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' agency', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' to', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' remain', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' contenders', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' in', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': ' the NFC', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '.', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'type': 'text', 'text': '', 'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[{'index': 0}], additional_kwargs={}, response_metadata={}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[], additional_kwargs={}, response_metadata={'stopReason': 'end_turn'}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a')}\n",
      "{'chunk': AIMessageChunk(content=[], additional_kwargs={}, response_metadata={'metrics': {'latencyMs': 7376}, 'model_name': 'us.amazon.nova-pro-v1:0'}, id='run--05865dcb-d838-405e-a1e8-350450e8e46a', usage_metadata={'input_tokens': 10, 'output_tokens': 732, 'total_tokens': 742})}\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "ename": "ThrottlingException",
     "evalue": "An error occurred (ThrottlingException) when calling the ConverseStream operation (reached max retries: 4): Too many requests, please wait before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mThrottlingException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m5\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m      2\u001b[39m input_message = HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mTell me about the 49ers NFL team\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph.astream_events({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [input_message]}, config, version=\u001b[33m\"\u001b[39m\u001b[33mv2\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Get chat model tokens from a particular node \u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event[\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mon_chat_model_stream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m event[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mlanggraph_node\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) == node_to_stream:\n\u001b[32m      6\u001b[39m         data = event[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:1404\u001b[39m, in \u001b[36mRunnable.astream_events\u001b[39m\u001b[34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[32m   1405\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:1021\u001b[39m, in \u001b[36m_astream_events_implementation_v2\u001b[39m\u001b[34m(runnable, value, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[39m\n\u001b[32m   1019\u001b[39m \u001b[38;5;66;03m# Await it anyway, to run any cleanup code, and propagate any exceptions\u001b[39;00m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.suppress(asyncio.CancelledError):\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m task\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:976\u001b[39m, in \u001b[36m_astream_events_implementation_v2.<locals>.consume_astream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;66;03m# if astream also calls tap_output_aiter this will be a no-op\u001b[39;00m\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(runnable.astream(value, config, **kwargs)) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m event_streamer.tap_output_aiter(run_id, stream):\n\u001b[32m    977\u001b[39m             \u001b[38;5;66;03m# All the content will be picked up\u001b[39;00m\n\u001b[32m    978\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:181\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    179\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/utils/aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2653\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2654\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2657\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2659\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2660\u001b[39m ):\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:181\u001b[39m, in \u001b[36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[39m\u001b[34m(self, run_id, output)\u001b[39m\n\u001b[32m    179\u001b[39m tap = \u001b[38;5;28mself\u001b[39m.is_tapped.setdefault(run_id, sentinel)\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m first = \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default=sentinel)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/utils/aiter.py:78\u001b[39m, in \u001b[36mpy_anext.<locals>.anext_impl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manext_impl\u001b[39m() -> Union[T, Any]:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[34m__anext__\u001b[39m(iterator)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:1471\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1468\u001b[39m final: Input\n\u001b[32m   1469\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1472\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1473\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1474\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1475\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1476\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1477\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1478\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1480\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:1471\u001b[39m, in \u001b[36mRunnable.atransform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1468\u001b[39m final: Input\n\u001b[32m   1469\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[32m   1472\u001b[39m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[32m   1473\u001b[39m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[32m   1474\u001b[39m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[32m   1475\u001b[39m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[32m   1476\u001b[39m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[32m   1477\u001b[39m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[32m   1478\u001b[39m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[32m   1480\u001b[39m         final = ichunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/base.py:1034\u001b[39m, in \u001b[36mRunnable.astream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mastream\u001b[39m(\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,  \u001b[38;5;66;03m# noqa: A002\u001b[39;00m\n\u001b[32m   1019\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1020\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   1021\u001b[39m ) -> AsyncIterator[Output]:\n\u001b[32m   1022\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Default implementation of astream, which calls ainvoke.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \n\u001b[32m   1024\u001b[39m \u001b[33;03m    Subclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1032\u001b[39m \u001b[33;03m        The output of the Runnable.\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langgraph/utils/runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/config.py:616\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    615\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    617\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    618\u001b[39m         cast(\u001b[33m\"\u001b[39m\u001b[33mCallable[..., T]\u001b[39m\u001b[33m\"\u001b[39m, partial(copy_context().run, wrapper)),\n\u001b[32m    619\u001b[39m     )\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/runnables/config.py:607\u001b[39m, in \u001b[36mrun_in_executor.<locals>.wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m() -> T:\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    609\u001b[39m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[32m    610\u001b[39m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[32m    611\u001b[39m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[32m    612\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mcall_model\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m     messages = state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: response}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1011\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream(\n\u001b[32m   1006\u001b[39m     async_api=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1007\u001b[39m     run_manager=run_manager,\n\u001b[32m   1008\u001b[39m     **kwargs,\n\u001b[32m   1009\u001b[39m ):\n\u001b[32m   1010\u001b[39m     chunks: \u001b[38;5;28mlist\u001b[39m[ChatGenerationChunk] = []\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_gen_info_and_msg_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_aws/chat_models/bedrock.py:604\u001b[39m, in \u001b[36mChatBedrock._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream\u001b[39m(\n\u001b[32m    597\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    598\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    601\u001b[39m     **kwargs: Any,\n\u001b[32m    602\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.beta_use_converse_api:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._as_converse._stream(\n\u001b[32m    605\u001b[39m             messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m    606\u001b[39m         )\n\u001b[32m    607\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    608\u001b[39m     provider = \u001b[38;5;28mself\u001b[39m._get_provider()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/langchain_aws/chat_models/bedrock_converse.py:618\u001b[39m, in \u001b[36mChatBedrockConverse._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    614\u001b[39m bedrock_messages, system = _messages_to_bedrock(messages)\n\u001b[32m    615\u001b[39m params = \u001b[38;5;28mself\u001b[39m._converse_params(\n\u001b[32m    616\u001b[39m     stop=stop, **_snake_to_camel_keys(kwargs, excluded_keys={\u001b[33m\"\u001b[39m\u001b[33minputSchema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    617\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverse_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbedrock_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m added_model_name = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/botocore/client.py:570\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    567\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m     )\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/botocore/context.py:124\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    123\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/WorkDocsDrive-Documents/WWSO-AIML/Code/lang-graph-course/langchain-academy/lc-academy-env/lib/python3.11/site-packages/botocore/client.py:1031\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1027\u001b[39m     error_code = error_info.get(\u001b[33m\"\u001b[39m\u001b[33mQueryErrorCode\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\n\u001b[32m   1028\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m     )\n\u001b[32m   1030\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mThrottlingException\u001b[39m: An error occurred (ThrottlingException) when calling the ConverseStream operation (reached max retries: 4): Too many requests, please wait before trying again.",
      "During task with name 'conversation' and id '2008b7a0-31e9-4377-af03-cb16ec129887'"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "**âš ï¸ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- ðŸš€ API: http://127.0.0.1:2024\n",
    "- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ðŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2818d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lc-academy-env)",
   "language": "python",
   "name": "lc-academy-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
